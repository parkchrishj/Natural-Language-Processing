{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of hwk2.ipynb","provenance":[{"file_id":"10yCXpnDyc0Tqq324lvXcTgqPtq_U3i8U","timestamp":1633714426585}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"widgets":{"application/vnd.jupyter.widget-state+json":{"32da67814a534b849400a298444a7ad2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_99a31591af2f441ca399ca3ab6d2ceb7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_66333b77fd044a2086ab6fed149b72f5","IPY_MODEL_8505c655c3bc4660a820f2c0d317567a","IPY_MODEL_2b29aff2b3ba419694f6d63811c940f3"]}},"99a31591af2f441ca399ca3ab6d2ceb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"66333b77fd044a2086ab6fed149b72f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_26079b3e245b4735acee8b8422916e59","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5294bf920beb4313bb9777521739bc95"}},"8505c655c3bc4660a820f2c0d317567a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_253b1a91922c4fe9976a71480258a7d4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":20,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":20,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2ce87df0b505445a9a9ee22d4e11e888"}},"2b29aff2b3ba419694f6d63811c940f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_94cd0505a98a4ec0889ef2085b6bef64","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 20/20 [32:51&lt;00:00, 98.21s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e35a9caa48b74189b7c98bab24bdfac3"}},"26079b3e245b4735acee8b8422916e59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5294bf920beb4313bb9777521739bc95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"253b1a91922c4fe9976a71480258a7d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2ce87df0b505445a9a9ee22d4e11e888":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"94cd0505a98a4ec0889ef2085b6bef64":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e35a9caa48b74189b7c98bab24bdfac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"15addd8427904f39932d17637bbf543f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c8126a3324a34fb4bb4f7c4e1cd8b1a1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d36f699f30d541e582f8303ae7fb8f52","IPY_MODEL_a998687e8ec74d14b93f7e3f4de5ee8b","IPY_MODEL_74b7997e693342d58648043843a5c4a9"]}},"c8126a3324a34fb4bb4f7c4e1cd8b1a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d36f699f30d541e582f8303ae7fb8f52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c2c464cb7f5a445f84ae6bebaebb6a89","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_940b7bcfb06848129b6bed431966959d"}},"a998687e8ec74d14b93f7e3f4de5ee8b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0862b582a18446389a395dd4b50d6664","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":5000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1e11cc8532c748458486f985f4390018"}},"74b7997e693342d58648043843a5c4a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3ce9f5266ab448ef8ff080fd7ae76bf9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5000/5000 [00:29&lt;00:00, 176.74it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_69ccdb2700a94448988d7481006911b4"}},"c2c464cb7f5a445f84ae6bebaebb6a89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"940b7bcfb06848129b6bed431966959d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0862b582a18446389a395dd4b50d6664":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1e11cc8532c748458486f985f4390018":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ce9f5266ab448ef8ff080fd7ae76bf9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"69ccdb2700a94448988d7481006911b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"336a98b6495840f59b0498b39b6d0036":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_92d97e76363641d999193d89a1e5bba8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e146bfa504e94bf2a32b83f2b6caa9a3","IPY_MODEL_b61c99316729458a806f242e5d04037f","IPY_MODEL_9ce61878c63d49f0a558dd261214af53"]}},"92d97e76363641d999193d89a1e5bba8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e146bfa504e94bf2a32b83f2b6caa9a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a92b423aaf7b4a46a9fb0d7fe924598d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6e19338c3a4b442990ababb730557dd1"}},"b61c99316729458a806f242e5d04037f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1bb33dfdefe34565a88e77fa60d7938e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":15,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":15,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_efce1672901549f7982dc343f92329aa"}},"9ce61878c63d49f0a558dd261214af53":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a628d32a8ffc45cf87efd769f9a2252d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 15/15 [1:00:43&lt;00:00, 243.58s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_00945f895a9f4e9d8d03200c3b5f9ad1"}},"a92b423aaf7b4a46a9fb0d7fe924598d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6e19338c3a4b442990ababb730557dd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1bb33dfdefe34565a88e77fa60d7938e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"efce1672901549f7982dc343f92329aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a628d32a8ffc45cf87efd769f9a2252d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"00945f895a9f4e9d8d03200c3b5f9ad1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1b3213bbc4ca47b7af3f4a2a4d6fd833":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d4f1191a0b5f4c82a7f611cc41f2f42e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_aa8fc2a3001b46798e730ede0bf6d8d2","IPY_MODEL_85c40bdafd7a4a6aa38ad04edfc44cf2","IPY_MODEL_0c494cb905ab4dd0a492d40d137504b8"]}},"d4f1191a0b5f4c82a7f611cc41f2f42e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aa8fc2a3001b46798e730ede0bf6d8d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4c6fff20ce1b4016912229ecddd3a10b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6cef71ff599448308bf7dce98165b673"}},"85c40bdafd7a4a6aa38ad04edfc44cf2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9489ea9684c443899620a08ce0beab4c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":5000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_067d589352974a1da4dff5b4f5c3e17e"}},"0c494cb905ab4dd0a492d40d137504b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e4c2ee9c2a5240eab19c7aedc69edf27","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5000/5000 [03:29&lt;00:00, 23.37it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b3b087d568f743daa5d5711417ecf2f8"}},"4c6fff20ce1b4016912229ecddd3a10b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6cef71ff599448308bf7dce98165b673":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9489ea9684c443899620a08ce0beab4c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"067d589352974a1da4dff5b4f5c3e17e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e4c2ee9c2a5240eab19c7aedc69edf27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b3b087d568f743daa5d5711417ecf2f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"EyCOvTRQ1nb-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634224283026,"user_tz":300,"elapsed":26741,"user":{"displayName":"Heejun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02999255376817400603"}},"outputId":"a4e1a33a-2d52-457a-e249-ff54b77c0cc7"},"source":["# Don't import any other libraries\n","from collections import defaultdict\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils import data\n","import torchtext \n","import random\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","if __name__=='__main__':\n","    print('Using device:', device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"markdown","metadata":{"id":"pSy-sfxOsclS"},"source":["# CS 447 Homework 2 $-$ Text Clasification with Neural Networks\n","In this homework, you will build machine learning models to detect the sentiment of movie reviews using the IMDb movie reviews dataset. Specifically, you will implement classifiers based on Convolutional Neural Networks (CNN's) and Recurrent Neural Networks (RNN's).\n","\n","We highly recommend that you take a look at the PyTorch tutorials before starting this assignment:\n","<ul>\n","<li>https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n","<li>https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n","<li>https://github.com/yunjey/pytorch-tutorial\n","</ul>\n","\n","We suggest that you select \"GPU\" as your runtime type, as this will speed up the training of your models. You can find this by going to <TT>Runtime > Change Runtime Type</TT> and select \"GPU\" from the dropdown menu."]},{"cell_type":"markdown","metadata":{"id":"zHbJ1-aDsWCG"},"source":["# Step 1: Download the Data\n","First we will download the dataset using [torchtext](https://torchtext.readthedocs.io/en/latest/index.html), which is a package that supports NLP for PyTorch. The following cell will get you `train_data` and `test_data`. It also does some basic tokenization.\n","\n","*   To access the list of textual tokens for the *i*th example, use `train_data[i][1]`\n","*   To access the label for the *i*th example, use `train_data[i][0]`\n","\n"]},{"cell_type":"code","metadata":{"id":"dfX3bNby8FYL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634224323001,"user_tz":300,"elapsed":35944,"user":{"displayName":"Heejun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02999255376817400603"}},"outputId":"b8ae2a74-4e57-4012-8efc-1e9b8a1c9dc6"},"source":["### DO NOT EDIT ###\n","\n","def preprocess(review):\n","    '''\n","    Simple preprocessing function.\n","    '''\n","    res = []\n","    for x in review.split(' '):\n","        remove_beg=True if x[0] in {'(', '\"', \"'\"} else False\n","        remove_end=True if x[-1] in {'.', ',', ';', ':', '?', '!', '\"', \"'\", ')'} else False\n","        if remove_beg and remove_end: res += [x[0], x[1:-1], x[-1]]\n","        elif remove_beg: res += [x[0], x[1:]]\n","        elif remove_end: res += [x[:-1], x[-1]]\n","        else: res += [x]\n","    return res\n","\n","if __name__=='__main__':\n","    train_data = torchtext.datasets.IMDB(root='.data', split='train')\n","    train_data = list(train_data)\n","    train_data = [(x[0], preprocess(x[1])) for x in train_data]\n","    train_data, test_data = train_data[0:10000] + train_data[12500:12500+10000], train_data[10000:12500] + train_data[12500+10000:], \n","\n","    print('Num. Train Examples:', len(train_data))\n","    print('Num. Test Examples:', len(test_data))\n","\n","    print(\"\\nSAMPLE DATA:\")\n","    for x in random.sample(train_data, 5):\n","        print('Sample text:', x[1])\n","        print('Sample label:', x[0], '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:02<00:00, 28.8MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Num. Train Examples: 20000\n","Num. Test Examples: 5000\n","\n","SAMPLE DATA:\n","Sample text: ['While', 'I', 'recently', 'gave', 'OPERATION', 'PETTICOAT', 'a', 'positive', 'review', ',', 'I', 'really', \"didn't\", 'like', 'this', 'film', 'even', 'though', 'it', 'had', 'so', 'many', 'similarities', '.', 'Both', 'were', 'made', 'just', 'a', 'few', 'years', 'apart', ',', 'both', 'starred', 'Cary', 'Grant', 'and', 'both', 'were', 'WWII', 'comedies', '.', 'However', ',', 'the', 'overall', 'tone', 'and', 'style', 'of', 'the', 'films', 'were', 'quite', 'different', '.', 'KISS', 'HER', 'FOR', 'ME', ',', 'overall', ',', 'just', 'seemed', 'like', 'a', 'cheaper', 'film--with', 'poor', 'writing', ',', 'little', 'energy', 'and', 'some', 'VERY', 'broad', 'performances--even', 'when', 'compared', 'to', 'OPERATION', 'PETTICOAT', '.', 'I', 'think', 'that', 'at', 'least', 'much', 'of', 'the', 'blame', 'for', 'this', 'lies', 'in', 'casting', 'Jayne', 'Mansfield', '.', 'The', 'combination', 'of', 'her', 'ample', 'talents', 'and', 'limited', 'acting', 'ability', 'really', 'made', 'this', 'A-budget', 'film', 'look', 'like', 'it', 'came', 'from', 'a', '3rd', 'rate', 'studio', '.', 'Plus', ',', 'there', \"wasn't\", 'much', 'chemistry', 'or', 'energy', 'in', 'pairing', 'her', 'with', 'Cary', 'Grant--an', 'actor', 'generally', 'loved', 'for', 'his', 'grace', 'and', 'class', '.', \"It's\", 'sort', 'of', 'like', 'pairing', 'Sir', 'Lawrence', 'Olivier', 'with', 'Marjorie', 'Main', '.']\n","Sample label: neg \n","\n","Sample text: ['Well', ',', 'well....Roeg', 'touched', 'a', 'bit', 'of', 'a', 'nerve', 'there', ',', \"didn't\", 'he', '?', 'He', 'was', 'a', 'genius', 'while', 'he', 'was', 'cataloguing', 'his', 'various', 'characters', \"'\", 'descents', 'into', 'psychosis', 'for', 'a', 'couple', 'of', 'decades', ',', 'but', 'as', 'soon', 'as', 'he', 'has', 'the', 'bad', 'taste', 'to', 'suggest', 'that', 'redemption', '(', 'or', 'even', 'some', 'good', 'advice', ')', 'might', 'be', 'found', 'in', 'the', 'bad', 'old', 'Catholic', 'church', ',', 'the', 'hipper-than-thou', 'alternative', 'movie', 'crowd', 'gets', 'extra', 'vicious', '.', 'Worse', 'still', ',', 'Theresa', \"Russell's\", 'character', '-', 'faced', 'with', 'experiences', 'that', 'nothing', 'in', 'her', 'avowedly', 'rationalist', 'outlook', 'has', 'an', 'explanation', 'for', ',', 'is', 'unwillingly', 'forced', 'to', 'deal', 'with', 'those', 'experiences', 'on', 'another', 'level', '-', 'that', 'of', 'the', 'spiritual', '.', 'You', 'know', ',', 'the', 'realm', 'of', 'the', 'ignorant', 'and', 'superstitious', ',', 'the', 'sort', 'of', 'thing', 'that', 'the', 'art-house', 'cinephiles', 'are', 'supposed', 'to', 'be', 'above', '.', 'Oh', ',', 'the', 'horror..', '.', 'So', 'she', 'finds', 'her', 'marriage', '-', 'the', 'idea', 'that', 'it', 'might', 'be', 'a', 'uniquely', 'important', 'commitment', '-', 'affirmed', 'by', 'what', 'seems', 'uncomfortably', 'like', 'divine', 'intervention', '.', 'People', 'who', 'find', 'this', 'idea', 'prima', 'facie', 'offensive', 'could', 'maybe', 'ask', 'themselves', 'why', 'they', 'instinctively', 'jump', 'into', 'attack', 'mode', 'at', 'being', 'challenged', 'to', 'take', 'seriously', 'the', 'idea', 'of', 'a', 'spiritual', 'dimension', 'to', 'their', 'lives', '.', 'But', 'they', 'probably', \"won't\", '.', 'Sure', ',', 'this', 'film', 'has', 'some', 'problems', ',', 'notably', 'Talia', \"Shire's\", 'delirious', 'hamwork', 'as', 'the', 'overwrought', 'nun', ',', '1950s-style', 'attire', 'and', 'all', '.', 'And', 'the', 'dialogue', 'between', 'Marie', 'Davenport', 'and', 'the', 'young', 'priest', 'in', 'their', 'last', 'scene', 'is', 'straight', 'out', 'of', 'the', 'Spellbound', 'School', 'of', 'Glib', 'Interpretations', '(', 'though', \"Hitchcock's\", 'movie', 'escaped', 'similar', 'charges', 'due', 'to', 'the', 'source', 'of', 'wisdom', 'having', 'impeccably', 'secular', 'credentials', 'as', 'a', 'Freudian', 'psychoanalyst)', '.', 'But', ',', 'sadly', ',', 'Nicolas', 'Roeg', 'appears', 'to', 'have', 'copped', 'a', 'critical', 'mauling', 'as', 'much', 'for', 'even', 'asking', 'the', 'question', 'as', 'for', 'the', 'possible', 'answers', 'this', 'film', 'presents', '.']\n","Sample label: pos \n","\n","Sample text: ['The', 'movie', 'held', 'my', 'interest', ',', 'mainly', 'because', 'Dianne', 'Keaton', 'is', 'my', 'favorite', 'actress', '.', 'I', 'disagree', 'with', 'some', 'of', 'the', 'other', 'posts', 'on', 'the', 'grounds', 'that', 'the', 'plot', 'was', 'not', 'convoluted', '.', 'I', 'had', 'no', 'trouble', 'following', 'it', '(', 'maybe', 'some', 'people', 'had', 'too', 'much', 'eggnog', 'the', 'night', 'before)', '.', 'The', 'movie', 'was', 'very', 'sad', 'and', 'touching', 'as', 'well', '.', 'What', 'more', 'do', 'you', 'want', '?', 'Alexa', 'Davalos', 'is', 'a', 'fine', 'new', 'talent', '(', 'beautiful', 'too)', ',', 'and', 'Tom', 'Everett', 'Scott', 'does', 'an', 'excellent', 'job', 'with', 'his', 'part', 'as', 'well', '.', 'The', 'relationship', 'of', 'the', 'mother', 'and', 'daughter', 'may', 'have', 'been', 'a', 'bit', 'unrealistic', ',', 'but', 'the', 'behavior', 'of', 'the', 'young', 'people', 'in', 'the', 'movie', 'was', 'not', '.', 'It', 'was', 'tragically', 'sad', 'but', 'enlightening', '.', 'It', 'sure', 'beat', 'the', 'other', 'shows', 'that', 'were', 'on', 'TV', 'New', 'Years', 'Day', 'evening']\n","Sample label: pos \n","\n","Sample text: ['I', 'stopped', 'by', 'BB', 'and', 'picked', 'up', '4', 'zombie', 'flicks', 'to', 'watch', 'over', 'the', 'weekend', '.', 'Now', ',', 'I', 'understand', 'that', 'the', 'effects', 'will', 'be', 'cheesy', ',', 'the', 'acting', 'will', 'be', 'sub-par', ',', 'and', 'the', 'sets', 'will', 'be', 'suspect', '.', 'So', \"I'm\", 'not', 'expecting', 'much', '.', 'But', 'it', 'should', 'at', 'least', 'have', 'a', 'story', '.', 'Stories', \"don't\", 'cost', 'a', 'thing', 'except', 'time.....apparently', ',', 'they', \"didn't\", 'have', 'any', 'time', 'either.<br', '/><br', '/>\"Zombie', 'Nation', '\"', 'had', '5', 'zombies', 'that', 'appeared', 'near', 'the', 'end', 'of', 'the', 'movie', 'that', 'all', 'looked', 'like', 'new', 'wave', 'hookers', '.', 'The', 'picture', 'of', 'the', 'zombie', 'on', 'the', 'front', 'cover', 'NEVER', 'appears', 'in', 'the', 'movie', '.', 'It', 'was', 'absolutely', 'agonizing', 'to', 'watch', 'and', 'had', 'nothing', 'to', 'offer', 'the', 'genre.<br', '/><br', '/>The', 'running', 'time', 'is', 'only', '81', 'minutes', 'but', 'it', 'felt', 'like', '2', 'hours', '.', 'According', 'to', 'my', 'wife', '(', 'who', 'could', 'only', 'hear', 'the', 'movie', 'since', 'she', 'was', 'on', 'the', 'computer', 'in', 'another', 'room)', ',', 'it', 'sounded', 'like', 'zombie', 'porn....which', 'if', 'you', 'think', 'about', ',', 'sounds', 'kinda', 'gross.....but', 'it', \"wasn't\", 'even', 'that', 'good.<br', '/><br', '/>The', 'only', 'suggestion', 'I', 'can', 'make', 'is', 'that', 'maybe', 'the', 'writer', 'tried', 'to', 'do', 'too', 'many', 'things', 'and', 'ended', 'up', 'with', 'an', 'incoherent', 'mess.<br', '/><br', '/>It', 'ended', 'up', 'being', 'a', 'free', 'rental', 'and', 'I', 'still', 'feel', 'ripped', 'off', '.', 'I', 'rated', 'it', 'a', '1', 'out', 'of', '10', 'because', 'IMDb', \"won't\", 'allow', 'me', 'to', 'use', 'decimals', '.']\n","Sample label: neg \n","\n","Sample text: [\"It's\", 'here', '.', 'finally', 'a', 'movie', 'comes', 'out', 'that', 'I', 'can', 'honestly', 'say', 'is', 'worse', 'than', 'Larry', 'the', 'cable', 'guy', ':', 'health', 'inspector', '.', 'Yet', \"I'm\", 'willing', 'to', 'bet', 'the', 'the', \"wayan's\", 'brothers(hilarious', ')', 'will', 'make', 'more', 'money', 'than', 'I', 'ever', 'make', 'in', 'my', 'whole', 'life', 'on', 'what', 'is', 'sure', 'to', 'be', 'one', 'of', 'the', 'top', 'five', 'worst', 'films', 'of', 'all', 'time', ',', 'outside', 'of', 'my', 'fifth', 'grade', 'outside', 'the', 'class', 're-enactment', 'of', 'romeo', 'and', 'Juliet', '.', 'I', 'mean', 'really', 'WHY', 'would', 'anyone', 'ever', 'ever', 'see', 'this', 'movie', 'unless', 'they', 'were', 'paid', 'to', '.', 'The', 'comedy', 'is', 'weak', 'and', 'all', 'even', 'remotely', 'funny', 'jokes', 'from', 'the', 'flimsy', 'plot', 'were', 'surely', 'revealed', 'in', 'commercials', '.', 'Final', 'word', 'is', 'this', 'movie', 'was', 'a', 'terrible', 'letdown', 'for', 'me', '.', 'And', 'the', 'commercials', 'looked', 'so', 'promising..', '.']\n","Sample label: neg \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"_kfg8RcyskyU"},"source":["# Step 2: Create Dataloader [20 points]\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lvFX-iX5oq7T"},"source":["## <font color='red'>TODO:</font> Define the Dataset Class [20 Points]\n","\n","In the following cell, we will define the <b>dataset</b> class. The dataset contains the tokenized data for your model. You need to implement the following functions: \n","\n","*   <b>` build_dictionary(self)`:</b>  <b>[10 points]</b> Creates the dictionaries `idx2word` and `word2idx`. You will represent each word in the dataset with a unique index, and keep track of this in these dictionaries. Use the hyperparameter `threshold` to control which words appear in the dictionary: a training word’s frequency should be `>= threshold` to be included in the dictionary.\n","\n","* <b>`convert_text(self)`:</b> Converts each review in the dataset to a list of indices, given by your `word2idx` dictionary. You should store this in the `textual_ids` variable, and the function does not return anything. If a word is not present in the  `word2idx` dictionary, you should use the `<UNK>` token for that word. Be sure to append the `<END>` token to the end of each review.\n","\n","*   <b>` get_text(self, idx) `:</b> Return the review at `idx` in the dataset as an array of indices corresponding to the words in the review. If the length of the review is less than `max_len`, you should pad the review with the `<PAD>` character up to the length of `max_len`. If the length is greater than `max_len`, then it should only return the first `max_len` words. The return type should be `torch.LongTensor`.\n","\n","*   <b>`get_label(self, idx) `</b>: Return the value `1` if the label for `idx` in the dataset is `positive`, and should return `0` if it is `negative`. The return type should be `torch.LongTensor`.\n","\n","*  <b> ` __len__(self) `:</b> Return the total number of reviews in the dataset as an `int`.\n","\n","*   <b>` __getitem__(self, idx)`:</b> <b>[10 points]</b> Return the (padded) text, and the label. The return type for both these items should be `torch.LongTensor`. You should use the ` get_label(self, idx) ` and ` get_text(self, idx) ` functions here.\n","\n","\n","<b>Note:</b> You should convert all words to lower case in your functions.\n","\n","<b>Autograder Hint:</b> Make sure that you use instance variables such as `self.threshold` throughout your code, rather than the global variable `THRESHOLD` (defined later on). The variable `THRESHOLD` will not be known to the autograder, and the use of it within the class will cause an autograder error."]},{"cell_type":"code","metadata":{"id":"1irMn3LX2YDB"},"source":["PAD = '<PAD>'\n","END = '<END>'\n","UNK = '<UNK>'\n","\n","class TextDataset(data.Dataset):\n","    def __init__(self, examples, split, threshold, max_len, idx2word=None, word2idx=None):\n","\n","        self.examples = examples #this is the train data i guess\n","        assert split in {'train', 'val', 'test'}\n","        self.split = split #split is 'train' doesn't really matter here\n","        self.threshold = threshold # threshold is 10\n","        self.max_len = max_len #max length is 150\n","\n","        # Dictionaries\n","        self.idx2word = idx2word #currently it's None\n","        self.word2idx = word2idx #currently it's None\n","        if split == 'train':\n","            self.build_dictionary() #calls the build_dictionary function\n","        self.vocab_size = len(self.word2idx) #How is this 3?\n","        \n","        # Convert text to indices\n","        self.textual_ids = [] #sorry what is this?\n","        self.convert_text()\n","\n","    \n","    def build_dictionary(self): \n","        '''\n","        Build the dictionaries idx2word and word2idx. This is only called when split='train', as these\n","        dictionaries are passed in to the __init__(...) function otherwise. Be sure to use self.threshold\n","        to control which words are assigned indices in the dictionaries.\n","        Returns nothing.\n","        '''\n","        assert self.split == 'train'\n","        \n","        # Don't change this\n","        self.idx2word = {0:PAD, 1:END, 2: UNK}\n","        self.word2idx = {PAD:0, END:1, UNK: 2}\n","\n","        ##### TODO #####\n","        # Count the frequencies of all words in the training data (self.examples) [ [\"label\",['word','word2','word3',....]] , [\"label2\",['wordzzz','zzz',....]] ]\n","          # To access the list of textual tokens for the ith example, use train_data[i][1]\n","          # To access the label for the ith example, use train_data[i][0]\n","        length = len(self.examples)\n","        # print(len(self.examples))\n","        frequency = {}\n","        # print(self.examples[0][1])\n","        for i in range(length):\n","          for word in self.examples[i][1]:\n","            word = word.lower()\n","            if word in frequency:\n","              frequency[word] += 1\n","            else:\n","              frequency[word] = 1\n","        i = 3\n","        for word,freq in frequency.items():\n","          if freq >= self.threshold:\n","            self.word2idx[word] = i\n","            i += 1\n","        # self.word2idx = {key:val for key, val in frequency.items() if val >= self.threshold or key == PAD or key == END or key == UNK}\n","        # print(self.word2idx)\n","        # self.idx2word = dict([(value, key) for key, value in self.word2idx.items()])\n","        self.idx2word = {key:val for val,key in self.word2idx.items()}\n","        # print(self.idx2word)\n","        # Assign idx (starting from 3) to all words having word_freq >= self.threshold\n","        # Make sure you call word.lower() on each word to convert it to lowercase\n","    \n","    def convert_text(self):\n","        '''\n","        Convert each review in the dataset (self.examples) to a list of indices, given by self.word2idx.\n","        Store this in self.textual_ids; returns nothing.\n","        '''\n","        #so would it look like this? [ [200,322,4444,4,135, ... max length], [0,1,23213,31,423 ... max length], ....]\n","        # print(self.word2idx['i'], self.word2idx['rented'],self.word2idx[UNK])\n","        length = len(self.examples)\n","        # indices_of_one_review = []\n","        for i in range(length):\n","          indices_of_one_review = []\n","          for review in self.examples[i][1]:\n","            review = review.lower()\n","            if review in self.word2idx:\n","              indices_of_one_review.append(self.word2idx[review])\n","            else:\n","              indices_of_one_review.append(self.word2idx[UNK])\n","          indices_of_one_review.append(self.word2idx[END])\n","          self.textual_ids.append(indices_of_one_review)\n","        # for i in range(20):\n","        #   print(self.textual_ids[0][i])\n","        # print(\"New one\")\n","        # for i in range(20):\n","        #   print(self.textual_ids[1][i])\n","            \n","\n","        ##### TODO #####\n","        # Remember to replace a word with the <UNK> token if it does not exist in the word2idx dictionary.\n","        # Remember to append the <END> token to the end of each review.\n","        pass\n","\n","    def get_text(self, idx):\n","        '''\n","        Return the review at idx as a long tensor (torch.LongTensor) of integers corresponding to the words in the review.\n","        You may need to pad as necessary (see above).\n","        '''\n","        #Return the review at idx in the dataset as an array of indices corresponding to the words in the review. \n","        #If the length of the review is less than max_len, you should pad the review with the <PAD> character up to the length of max_len. \n","        #If the length is greater than max_len, then it should only return the first max_len words.\n","        # query through the textual_ids  [4, 5, '<UNK>', 7, 8, 9, 10,'<END>','<PAD>','<PAD>']  [4, 5, '<UNK>', 7, 8] \n","        # the parameter is showing that idx is the index of the review of the textual_ids\n","        # query first then pytorch at the end\n","        ##### TODO #####\n","        length = len(self.textual_ids[idx])\n","        data = []\n","        for i in range(self.max_len):\n","          if i >= length:\n","            data.append(0)\n","          else:\n","            data.append(self.textual_ids[idx][i])\n","        data = torch.LongTensor(data)\n","        return data\n","    \n","    def get_label(self, idx):\n","        '''\n","        This function should return the value 1 if the label for idx in the dataset is 'positive', \n","        and 0 if it is 'negative'. The return type should be torch.LongTensor.\n","        '''\n","        ##### TODO #####\n","        # the parameter is showing that idx is the index of the review of the self.examples, and then if it's positive, reutnr\n","        # querying through self.examples. [0] or [1] just return torch.tensor(0) or torch.tensor(1)\n","        # To access the label for the ith example, use train_data[i][0]\n","        hi = torch.LongTensor([1])\n","        # hi = torch.LongTensor(1)\n","        # return hi tensor([94688258809728])\n","        # print(self.examples[idx][0])\n","        if self.examples[idx][0] == 'pos':\n","          hi = torch.LongTensor([1])\n","        if self.examples[idx][0] == 'neg':\n","          hi = torch.LongTensor([0])\n","        return torch.squeeze(hi) #tensor(94688258801856)\n","\n","    def __len__(self):\n","        '''\n","        Return the number of reviews (int value) in the dataset\n","        '''\n","        ##### TODO #####\n","        return len(self.examples)\n","    \n","    def __getitem__(self, idx):\n","        '''\n","        Return the review, and label of the review specified by idx.\n","        '''\n","        # Return the (padded) text, and the label. The return type for both these items should be torch.LongTensor. \n","        # You should use the get_label(self, idx) and get_text(self, idx) functions here.\n","        ##### TODO #####\n","        review = self.get_text(idx)\n","        label = self.get_label(idx)\n","        return review, label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSxpGXj6ml9N","executionInfo":{"status":"ok","timestamp":1634239761672,"user_tz":300,"elapsed":4491,"user":{"displayName":"Heejun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02999255376817400603"}},"outputId":"a5fbdf49-bf98-4fd4-e5b2-13198cdc066e"},"source":["if __name__=='__main__':\n","    # Sample item\n","    Ds = TextDataset(train_data, 'train', threshold=10, max_len=150)\n","    print(\"Ds\", Ds)\n","    print('Vocab size:', Ds.vocab_size)\n","    # print(len(Ds)) it's 20,000 but for some reason it keeps printing the same one??\n","    text, label = Ds[random.randint(0, len(Ds))]\n","    # text, label = Ds[1]\n","    print('Example text:', text)\n","    print('Example label:', label)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ds <__main__.TextDataset object at 0x7f04246aaf50>\n","Vocab size: 19002\n","Example text: tensor([ 1033,     2,     2,    22,  3306,   809,    40,     2,    28,    41,\n","         5514, 16161,    24,    68,    55,     2, 10801,   105,    13,   154,\n","           74, 11371,  1710,    74, 12217,    24,    88,   952,    38,     2,\n","            2,    38,    13, 11371,  8040,   148,   553,   251,     2,   638,\n","           91,  9555,    74,    34, 10067,  3525,    24,     2,    91,  3402,\n","           38, 18382,   681,    34,   384,   200,  4229,  2936,    80,    74,\n","        11371,    38,  6510,    13,  2707,     2,   325,    11,    13,  7931,\n","           91, 12112,   658,  7944,  3541,    34,     2, 10828,    22,   148,\n","         1029,    91,  8661,    24,  8535,  5683,    91, 12177,  1555,    38,\n","         1346,   721,    34,  2084,  1555,    34,     2,     2,     2,    91,\n","            2,  7387,   179,    18,   209,   384,   638,    24,   258,    55,\n","          631,    41,  5195,    11,  4874,    38,   180,    17,    55,   125,\n","         1784,     6, 18382,    24,   249,    68,  3841,    34,  9698,    74,\n","          736,  5565,   215,  3127,    38,  5045,  4238,    91,  6063,  3794,\n","          633,    50,   200,  7013,  1007,    24, 16124,    47,  1642,   310])\n","Example label: tensor(1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"e_4FFhulaAod"},"source":["# Step 3: Train a Convolutional Neural Network (CNN) [40 points]"]},{"cell_type":"markdown","metadata":{"id":"VcSKydlClwOC"},"source":["## <font color='red'>TODO:</font> Define the CNN Model [20 points]\n","Here you will define your convolutional neural network for text classification. We provide you with the CNN class, you need to fill in parts of the `__init__(...)` and `forward(...)` functions. Each of these functions is worth 10 points.\n","\n","We have provided you with instructions and hints in the comments. In particular, pay attention to the desired shapes; you may find it helpful to print the shape of the tensors as you code. It may also help to keep PyTorch documentation open for the modules & functions you are using, since they describe input and output dimensions."]},{"cell_type":"code","metadata":{"id":"0ztuy2hUaAof"},"source":["class CNN(nn.Module):\n","    def __init__(self, vocab_size, embed_size, out_channels, filter_heights, stride, dropout, num_classes, pad_idx):\n","        super(CNN, self).__init__()\n","        \n","        ##### TODO #####\n","        # Create an embedding layer (https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n","        #   to represent the words in your vocabulary. Make sure to use vocab_size, embed_size, and pad_idx here.\n","        \"\"\"\n","        CNN(vocab_size = train_Ds.vocab_size, # Don't change this\n","                embed_size = 128, \n","                out_channels = 64, \n","                filter_heights = [2, 3, 4], \n","                stride = 1, \n","                dropout = 0.5, \n","                num_classes = 2, # Don't change this\n","                pad_idx = train_Ds.word2idx[PAD]) # Don't change this\n","        \"\"\"\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim=embed_size, padding_idx = pad_idx)\n","\n","        # Define multiple Convolution layers (nn.Conv2d) with filter (kernel) size [filter_height, embed_size] based on your \n","        #   different filter_heights.\n","        # Input channels will be 1 and output channels will be out_channels (these many different filters will be trained \n","        #   for each convolution layer)\n","        # If you want, you can store a list of modules inside nn.ModuleList.\n","        # Note: even though your conv layers are nn.Conv2d, we are doing a 1d convolution since we are only moving the filter \n","        #   in one direction\n","        self.convs = nn.ModuleList([nn.Conv2d(in_channels = 1,  out_channels=out_channels, \n","                                              kernel_size= (fs,embed_size)) for fs in filter_heights])\n","        # Create a dropout layer (nn.Dropout) using dropout\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # Define a linear layer (nn.Linear) that consists of num_classes units \n","        #   and takes as input the concatenated output for all cnn layers (out_channels * num_of_cnn_layers units)\n","        self.fc = nn.Linear(len(filter_heights)*out_channels, num_classes)\n","\n","\n","    def forward(self, texts):\n","        \"\"\"\n","        texts: LongTensor [batch_size, max_len]\n","        \n","        Returns output: Tensor [batch_size, num_classes]\n","        \"\"\"\n","        ##### TODO #####\n","\n","        # Pass texts through your embedding layer to convert from word ids to word embeddings\n","        #   Resulting: shape: [batch_size, max_len, embed_size]\n","        embedded = self.embedding(texts)\n","\n","        # Input to conv should have 1 channel. Take a look at torch's unsqueeze() function\n","        #   Resulting shape: [batch_size, 1, MAX_LEN, embed_size]\n","        embedded = torch.unsqueeze(embedded, 1)\n","        \n","        # Pass these texts to each of your conv layers and compute their output as follows:\n","        #   Your cnn output will have shape [batch_size, out_channels, *, 1] where * depends on filter_height and stride\n","        #   Convert to shape [batch_size, out_channels, *] (see torch's squeeze() function)\n","        #   Apply non-linearity on it (F.relu() is a commonly used one. Feel free to try others)\n","        #   Take the max value across last dimension to have shape [batch_size, out_channels]\n","        # Concatenate (torch.cat) outputs from all your cnns [batch_size, (out_channels*num_of_cnn_layers)]\n","        #\n","        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n","        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n","        concat = torch.cat(pooled,dim=1)\n","\n","        # Let's understand what you just did:\n","        #   Since each cnn is of different filter_height, it will look at different number of words at a time\n","        #     So, a filter_height of 3 means your cnn looks at 3 words (3-grams) at a time and tries to extract some information from it\n","        #   Each cnn will learn out_channels number of features from the words it sees at a time\n","        #   Then you applied a non-linearity and took the max value for all channels\n","        #     You are essentially trying to find important n-grams from the entire text\n","        # Everything happens on a batch simultaneously hence you have that additional batch_size as the first dimension\n","        \n","        # Apply dropout\n","        cat = self.dropout(concat)\n","\n","        # Pass your output through the linear layer and return its output \n","        #   Resulting shape: [batch_size, num_classes]\n","        \n","\n","        ##### NOTE: Do not apply a sigmoid or softmax to the final output - done in training method!\n","\n","        return self.fc(cat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FupiBIfasCu_"},"source":["## Train CNN Model\n","\n","First, we initialize the train and test <b>dataloaders</b>. A dataloader is responsible for providing batches of data to your model. Notice how we first instantiate datasets for the train and test data, and that we use the training vocabulary for both.\n","\n","You do not need to edit this cell."]},{"cell_type":"code","metadata":{"id":"J2QYl334n9ON"},"source":["if __name__=='__main__':\n","    THRESHOLD = 5 # Don't change this\n","    MAX_LEN = 100 # Don't change this\n","    BATCH_SIZE = 32 # Feel free to try other batch sizes\n","\n","    train_Ds = TextDataset(train_data, 'train', THRESHOLD, MAX_LEN)\n","    train_loader = torch.utils.data.DataLoader(train_Ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n","\n","    test_Ds = TextDataset(test_data, 'test', THRESHOLD, MAX_LEN, train_Ds.idx2word, train_Ds.word2idx)\n","    test_loader = torch.utils.data.DataLoader(test_Ds, batch_size=1, shuffle=False, num_workers=1, drop_last=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AvsctopWmeoY"},"source":["Now we provide you with a function that takes your model and trains it on the data.\n","\n","You do not need to edit this cell. However, you may want to write code to save your model periodically, as Colab connections are not permanent. See the tutorial here if you wish to do this: https://pytorch.org/tutorials/beginner/saving_loading_models.html."]},{"cell_type":"code","metadata":{"id":"LD-Jj2rUFOzr"},"source":["### DO NOT EDIT ###\n","\n","from tqdm.notebook import tqdm\n","\n","def train_model(model, num_epochs, data_loader, optimizer, criterion):\n","    print('Training Model...')\n","    model.train()\n","    for epoch in tqdm(range(num_epochs)):\n","        epoch_loss = 0\n","        epoch_acc = 0\n","        for texts, labels in data_loader:\n","            texts = texts.to(device) # shape: [batch_size, MAX_LEN]\n","            labels = labels.to(device) # shape: [batch_size]\n","\n","            optimizer.zero_grad()\n","\n","            output = model(texts)\n","            acc = accuracy(output, labels)\n","            \n","            loss = criterion(output, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        print('[TRAIN]\\t Epoch: {:2d}\\t Loss: {:.4f}\\t Train Accuracy: {:.2f}%'.format(epoch+1, epoch_loss/len(data_loader), 100*epoch_acc/len(data_loader)))\n","    print('Model Trained!\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FyIZS0WUhFA6"},"source":["Here are some other helper functions we will need."]},{"cell_type":"code","metadata":{"id":"zVP2scuyhG5f"},"source":["### DO NOT EDIT ###\n","\n","def count_parameters(model):\n","    \"\"\"\n","    Count number of trainable parameters in the model\n","    \"\"\"\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def accuracy(output, labels):\n","    \"\"\"\n","    Returns accuracy per batch\n","    output: Tensor [batch_size, n_classes]\n","    labels: LongTensor [batch_size]\n","    \"\"\"\n","    preds = output.argmax(dim=1) # find predicted class\n","    correct = (preds == labels).sum().float() # convert into float for division \n","    acc = correct / len(labels)\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YjvX5c6Isw9e"},"source":["Now you can instantiate your model. We provide you with some recommended hyperparameters; you should be able to get the desired accuracy with these, but feel free to play around with them."]},{"cell_type":"code","metadata":{"id":"M5UtdjGDuBty","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634151361800,"user_tz":300,"elapsed":191,"user":{"displayName":"Heejun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02999255376817400603"}},"outputId":"9e0eb87b-a7c1-4b4c-9c9a-56e029d3506e"},"source":["if __name__=='__main__':\n","    cnn_model = CNN(vocab_size = train_Ds.vocab_size, # Don't change this\n","                embed_size = 128, \n","                out_channels = 64, \n","                filter_heights = [2, 3, 4], \n","                stride = 1, \n","                dropout = 0.5, \n","                num_classes = 2, # Don't change this\n","                pad_idx = train_Ds.word2idx[PAD]) # Don't change this\n","\n","    # Put your model on the device (cuda or cpu)\n","    cnn_model = cnn_model.to(device)\n","    \n","    print('The model has {:,d} trainable parameters'.format(count_parameters(cnn_model)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 3,879,746 trainable parameters\n"]}]},{"cell_type":"markdown","metadata":{"id":"SeHpqw6zvkhI"},"source":["Next, we create the **criterion**, which is our loss function: it is a measure of how well the model matches the empirical distribution of the data. We use cross-entropy loss (https://en.wikipedia.org/wiki/Cross_entropy).\n","\n","We also define the **optimizer**, which performs gradient descent. We use the Adam optimizer (https://arxiv.org/pdf/1412.6980.pdf), which has been shown to work well on these types of models."]},{"cell_type":"code","metadata":{"id":"FoeyQL4PoNoH","colab":{"base_uri":"https://localhost:8080/","height":207},"executionInfo":{"status":"error","timestamp":1634239740013,"user_tz":300,"elapsed":112,"user":{"displayName":"Heejun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02999255376817400603"}},"outputId":"6e112237-2d65-49e4-fa55-d3b41633b2be"},"source":["if __name__=='__main__':    \n","    LEARNING_RATE = 5e-4 # Feel free to try other learning rates\n","\n","    # Define the loss function\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    # Define the optimizer\n","    optimizer = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-253-f479996d421b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Define the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'cnn_model' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"RopLfAJ9wOHN"},"source":["Finally, we can train the model."]},{"cell_type":"code","metadata":{"id":"lPOs1FifoNoN","colab":{"base_uri":"https://localhost:8080/","height":494,"referenced_widgets":["32da67814a534b849400a298444a7ad2","99a31591af2f441ca399ca3ab6d2ceb7","66333b77fd044a2086ab6fed149b72f5","8505c655c3bc4660a820f2c0d317567a","2b29aff2b3ba419694f6d63811c940f3","26079b3e245b4735acee8b8422916e59","5294bf920beb4313bb9777521739bc95","253b1a91922c4fe9976a71480258a7d4","2ce87df0b505445a9a9ee22d4e11e888","94cd0505a98a4ec0889ef2085b6bef64","e35a9caa48b74189b7c98bab24bdfac3"]},"executionInfo":{"status":"ok","timestamp":1634153335957,"user_tz":300,"elapsed":1971359,"user":{"displayName":"Heejun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02999255376817400603"}},"outputId":"bb7f0ba2-4db0-4975-e256-d4905a275910"},"source":["if __name__=='__main__':    \n","    N_EPOCHS = 20 # Feel free to change this\n","    \n","    # train model for N_EPOCHS epochs\n","    train_model(cnn_model, N_EPOCHS, train_loader, optimizer, criterion)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Model...\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"32da67814a534b849400a298444a7ad2","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/20 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  1\t Loss: 0.6981\t Train Accuracy: 58.42%\n","[TRAIN]\t Epoch:  2\t Loss: 0.5865\t Train Accuracy: 68.44%\n","[TRAIN]\t Epoch:  3\t Loss: 0.5446\t Train Accuracy: 72.18%\n","[TRAIN]\t Epoch:  4\t Loss: 0.5006\t Train Accuracy: 75.36%\n","[TRAIN]\t Epoch:  5\t Loss: 0.4551\t Train Accuracy: 78.40%\n","[TRAIN]\t Epoch:  6\t Loss: 0.4130\t Train Accuracy: 80.87%\n","[TRAIN]\t Epoch:  7\t Loss: 0.3711\t Train Accuracy: 83.46%\n","[TRAIN]\t Epoch:  8\t Loss: 0.3245\t Train Accuracy: 85.84%\n","[TRAIN]\t Epoch:  9\t Loss: 0.2836\t Train Accuracy: 87.97%\n","[TRAIN]\t Epoch: 10\t Loss: 0.2408\t Train Accuracy: 89.96%\n","[TRAIN]\t Epoch: 11\t Loss: 0.2030\t Train Accuracy: 91.71%\n","[TRAIN]\t Epoch: 12\t Loss: 0.1680\t Train Accuracy: 93.39%\n","[TRAIN]\t Epoch: 13\t Loss: 0.1372\t Train Accuracy: 94.66%\n","[TRAIN]\t Epoch: 14\t Loss: 0.1181\t Train Accuracy: 95.50%\n","[TRAIN]\t Epoch: 15\t Loss: 0.0877\t Train Accuracy: 96.77%\n","[TRAIN]\t Epoch: 16\t Loss: 0.0836\t Train Accuracy: 96.98%\n","[TRAIN]\t Epoch: 17\t Loss: 0.0649\t Train Accuracy: 97.59%\n","[TRAIN]\t Epoch: 18\t Loss: 0.0543\t Train Accuracy: 98.14%\n","[TRAIN]\t Epoch: 19\t Loss: 0.0485\t Train Accuracy: 98.26%\n","[TRAIN]\t Epoch: 20\t Loss: 0.0429\t Train Accuracy: 98.55%\n","Model Trained!\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"Q-OJbZ72t6Yq"},"source":["## Evaluate CNN Model [20 points]\n","\n","Now that we have trained a model for text classification, it is time to evaluate it. We have provided you with a function to do this; you do not need to modify anything.\n","\n","To pass the autograder for the CNN, you will need to achieve **73% accuracy** on the test set. Note that Gradescope uses a different test set; however, it is very similar, and the accuracies between the two datasets should be comparable."]},{"cell_type":"code","metadata":{"id":"vTiiYDZIF--7"},"source":["### DO NOT EDIT ###\n","\n","import random\n","\n","def evaluate(model, data_loader, criterion):\n","    print('Evaluating performance on the test dataset...')\n","    model.eval()\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    all_predictions = []\n","    print(\"\\nSOME PREDICTIONS FROM THE MODEL:\")\n","    for texts, labels in tqdm(data_loader):\n","        texts = texts.to(device)\n","        labels = labels.to(device)\n","        \n","        output = model(texts)\n","        acc = accuracy(output, labels)\n","        pred = output.argmax(dim=1)\n","        all_predictions.append(pred)\n","        \n","        loss = criterion(output, labels)\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","        if random.random() < 0.0015:\n","            print(\"Input: \"+' '.join([data_loader.dataset.idx2word[idx] for idx in texts[0].tolist() if idx not in {data_loader.dataset.word2idx[PAD], data_loader.dataset.word2idx[END]}]))\n","            print(\"Prediction:\", pred.item(), '\\tCorrect Output:', labels.item(), '\\n')\n","\n","    full_acc = 100*epoch_acc/len(data_loader)\n","    full_loss = epoch_loss/len(data_loader)\n","    print('[TEST]\\t Loss: {:.4f}\\t Accuracy: {:.2f}%'.format(full_loss, full_acc))\n","    predictions = torch.cat(all_predictions)\n","    return predictions, full_acc, full_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z718w8e0oNoS","colab":{"base_uri":"https://localhost:8080/","height":749,"referenced_widgets":["15addd8427904f39932d17637bbf543f","c8126a3324a34fb4bb4f7c4e1cd8b1a1","d36f699f30d541e582f8303ae7fb8f52","a998687e8ec74d14b93f7e3f4de5ee8b","74b7997e693342d58648043843a5c4a9","c2c464cb7f5a445f84ae6bebaebb6a89","940b7bcfb06848129b6bed431966959d","0862b582a18446389a395dd4b50d6664","1e11cc8532c748458486f985f4390018","3ce9f5266ab448ef8ff080fd7ae76bf9","69ccdb2700a94448988d7481006911b4"]},"executionInfo":{"status":"ok","timestamp":1634153376437,"user_tz":300,"elapsed":29716,"user":{"displayName":"Heejun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02999255376817400603"}},"outputId":"f2e5c396-0d8b-4698-a6af-8373057a69d0"},"source":["if __name__=='__main__':\n","    evaluate(cnn_model, test_loader, criterion) # Compute test data accuracy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating performance on the test dataset...\n","\n","SOME PREDICTIONS FROM THE MODEL:\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15addd8427904f39932d17637bbf543f","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Input: why do all movies on lifetime have such anemic titles ? \" an unexpected love \" - ooh , how <UNK> ! \" this much i know \" would have been better . the film is nothing special . real people don't really talk like these characters do and the situations are really hackneyed . the straight woman who \" turns \" lesbian seemed more butch than the lesbian character . if you wanna watch two hot women kiss in a very discreet fashion , you might enjoy this . although it seems like it was written by someone who\n","Prediction: 0 \tCorrect Output: 0 \n","\n","Input: all internet buzz aside , this movie was god awful . i expected the movie to be more of a farce than anything . instead the film makers tried to make a serious thriller/horror movie , and they completely missed . there were only a few good parts , and a couple good lines by samuel jackson . other than that , it was a bunch of gore and some poorly animated snakes . all of the internet joking was miles better than the actual movie . now that the movie has actually come out , hopefully this joke will\n","Prediction: 0 \tCorrect Output: 0 \n","\n","Input: perhaps it's because i am so in love with the william holden - kim novak version , or because i'm not a <UNK> , but this was absolutely the worst remake i have ever seen . without the original's soundtrack , it just seemed like another typical tv <UNK> , about as bland as kraft cheese .\n","Prediction: 0 \tCorrect Output: 0 \n","\n","Input: and a few more \" <UNK> on top of that . voodoo academy is , without a doubt , the least ambitious film of all time . what exactly is it trying to do ? tell a story ? obviously not ; as has been pointed out , most of it's just <UNK> guys rubbing themselves . could it , then , be an attempt at subversive <UNK> ? well , maybe , if not for the fact it never ever ever goes beyond the most innocuous and <UNK> forms of male contact . ( which is , to the\n","Prediction: 0 \tCorrect Output: 0 \n","\n","Input: a gave it a \" 2 \" instead of a \" 1 \" ( awful ) because there is no denying that many of the visuals were stunning , a lot of talent went into the special effects and artwork . but that wasn't enough to save it.<br /><br />the \" sepia \" toned , washed out colors sort of thing has been done before many times in other movies . nothing new there . i can see there were some <UNK> to other old , classic movies . ok . no problem with that.<br /><br />but a movie has\n","Prediction: 0 \tCorrect Output: 0 \n","\n","Input: this movie purports to be a character study of perversion . some reviewers have been <UNK> into assuming that because perversion is depicted , the film is psychologically deep ; actually , considering the salacious material , it is surprisingly tedious and shallow , with no <UNK> substance . why is the main character the way she is ? you won't find out from the script . for a better treatment of the same theme ( and a more entertaining movie) , try bunuel's belle de jour .\n","Prediction: 0 \tCorrect Output: 0 \n","\n","Input: not even worth watching this tacky spoiler ruins everything about ' <UNK> . the characters seem almost <UNK> by the poorly written storyline and they low quality feeling to the production . it was very clearly made for tv , yet if i found it on my television , i would flick it straight over . the children in the film do an alright job , yet the adults acting is unbelievable and so the movie fails to really draw you in . this film lacked the <UNK> numbers thats made the original brilliant and truly does take the shine\n","Prediction: 0 \tCorrect Output: 0 \n","\n","Input: i saw the movie recently and really liked it . i surprised myself and cried . this movie is in the same niche genre as \" away from her \" - or even \" the bucket list \" but handles the whole aging theme with incredible authenticity . it's really really tough to have the main character as unlikable as hagar . the director does a masterful job with the challenge . hagar's hard to understand . her world has hard edges and she isn't a warm endearing woman at all.<br /><br />the first scene gets this across without any\n","Prediction: 1 \tCorrect Output: 1 \n","\n","Input: the sopranos is arguably the greatest show in dramatic television history.<br /><br />its hard to think of another series that boasts so much intelligence , sublime writing or first rate performances.<br /><br <UNK> its epic scope it produces fresh and iconic characters and a constant level of high quality . centering around the life of one tony soprano , a man who lives in two families . one is the conventional wife and two kids nuclear family the other a huge new jersey mafia group , of which he is the boss of both . played by james gandolfini ,\n","Prediction: 1 \tCorrect Output: 1 \n","\n","Input: i sat down to watch \" midnight cowboy \" thinking it would be another overrated ' <UNK> movie . some of my favorite films come from the ' 70s , in the same vein as \" midnight cowboy \" ( <UNK> <UNK> \" \" mean <UNK> \" \" panic in needle <UNK> \" etc. ) but there are many , many overrated ones as well that have gained strong reputations amongst critics for being groundbreaking - unfortunately a vast majority of them don't hold up as well today . i sort of feel this way about \" easy <UNK> \"\n","Prediction: 1 \tCorrect Output: 1 \n","\n","Input: that was definitely the case with angels in the outfield . it was on tv last night and i believe i hadn't seen the film since my sophomore year in high school and i'm now in my 4th year of college . although the film has many flaws , it is just so touching that you can't help but sit down , watch it , and enjoy yourself . it is also hilarious . danny glover's ranting is just so over the top that you can't help but laugh out loud at him at most time . it adds to\n","Prediction: 1 \tCorrect Output: 1 \n","\n","Input: i agree with another user here and have to say that this is one of the best kung fu movies ever ! i watched this as a kid and absolutely loved it ! the <UNK> scenes are brilliant and you can really empathise with this guy because he is treated as an outcast . nice humour and fantastic kung fu this movie rocks ! if you like kung fu you would love this!! !\n","Prediction: 1 \tCorrect Output: 1 \n","\n","[TEST]\t Loss: 1.1416\t Accuracy: 74.66%\n"]}]},{"cell_type":"markdown","metadata":{"id":"BRCFvjwDthiA"},"source":["# Step 4: Train a Recurrent Neural Network (RNN) [40 points]\n","You will now build a text clasification model that is based on **recurrences**."]},{"cell_type":"markdown","metadata":{"id":"Y-t8tlZviV2x"},"source":["## <font color='red'>TODO:</font> Define the RNN Model [20 points]\n","\n","First, you will define the RNN. As with the CNN, we provide you with the skeleton of the class, and you need to fill in parts of the `__init__(...)` and `forward(...)` methods. Each of these functions is worth 10 points."]},{"cell_type":"code","metadata":{"id":"2nc_HxbP6klI"},"source":["class RNN(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, bidirectional, dropout, num_classes, pad_idx):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bidirectional = bidirectional\n","\n","        ##### TODO #####\n","        # Create an embedding layer (https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n","        #   to represent the words in your vocabulary. Make sure to use vocab_size, embed_size, and pad_idx here.\n","        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx = pad_idx)\n","\n","        # Create a recurrent network (use nn.GRU, not nn.LSTM) with batch_first = True\n","        # Make sure you use hidden_size, num_layers, dropout, and bidirectional here.\n","        self.rnn = nn.GRU(embed_size, hidden_size, num_layers = num_layers, bidirectional = bidirectional, dropout = dropout, batch_first = True)\n","        \n","        # Create a dropout layer (nn.Dropout) using dropout\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Define a linear layer (nn.Linear) that consists of num_classes units \n","        #   and takes as input the output of the last timestep. In the bidirectional case, you should concatenate\n","        #   the output of the last timestep of the forward direction with the output of the last timestep of the backward direction).\n","        if self.bidirectional:\n","          self.fc = nn.Linear(hidden_size*2, num_classes)\n","        else:\n","          self.fc = nn.Linear(hidden_size, num_classes)\n","\n","\n","    def forward(self, texts):\n","        \"\"\"\n","        texts: LongTensor [batch_size, MAX_LEN]\n","        \n","        Returns output: Tensor [batch_size, num_classes]\n","        \"\"\"\n","        ##### TODO #####\n","        \"\"\"\n","        RNN(vocab_size = train_Ds.vocab_size, # Don't change this\n","                embed_size = 128, \n","                hidden_size = 128, \n","                num_layers = 2,\n","                bidirectional = True,\n","                dropout = 0.5,\n","                num_classes = 2, # Don't change this\n","                pad_idx = train_Ds.word2idx[PAD]) # Don't change this\n","        \"\"\"\n","        # Pass texts through your embedding layer to convert from word ids to word embeddings\n","        #   Resulting: shape: [batch_size, max_len, embed_size]\n","        # texts [batch_size, MAX_LEN] is torch.Size([32, 100])\n","        # embedding (vocabsize = 29730, 128, ...)\n","        embedded = self.embedding(texts) #torch.Size([32, 100, 128])\n","\n","        # Pass the result through your recurrent network\n","        #   See PyTorch documentation for resulting shape for nn.GRU\n","        output, hidden = self.rnn(embedded)#, 2 if self.bidirectional else 1)\n","        # output is torch.Size([4, 32, 128])\n","        # hidden is torch.Size([32, 100, 256])\n","\n","        #hidden is the final hidden state for each element in the batch\n","        # output features (h_t) from the last layer of the GRU, for each t.\n","        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(output)\n","\n","        # Concatenate the outputs of the last timestep for each direction (see torch.cat(...))\n","        #   This depends on whether or not your model is bidirectional.\n","        #   Resulting shape: [batch_size, num_dirs*hidden_size]\n","        # concatenate from the last hidden GRU. layer which is the output o.. hidden is already the last timestep of GRU\n","        # extract last timestep for each direction if bidirectional, concatenate, else don't\n","        if self.bidirectional:\n","          dropped = self.dropout(torch.cat((hidden[-2,:,:],hidden[-1,:,:]), dim = -1))\n","          #concat = torch.cat([hidden[-1], hidden[-2], dim = -1])\n","        else:\n","          dropped = self.dropout(hidden[-1,:,:])\n","          #dropped = self.dropout(hidden[-1])\n","        # Apply dropout\n","        dropped = self.dropout(dropped)\n","\n","\n","        # # Pass your output through the linear layer and return its output \n","        # #   Resulting shape: [batch_size, num_classes]\n","        prediction = self.fc(dropped)\n","\n","        ##### NOTE: Do not apply a sigmoid or softmax to the final output - done in training method!\n","        \n","        return prediction"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"baD8lYAytdTV"},"source":["## Train RNN Model\n","First, we initialize the train and test dataloaders."]},{"cell_type":"code","metadata":{"id":"WCzNm8LDM5aT"},"source":["if __name__=='__main__':\n","    THRESHOLD = 5 # Don't change this\n","    MAX_LEN = 100 # Don't change this\n","    BATCH_SIZE = 32 # Feel free to try other batch sizes\n","\n","    train_Ds = TextDataset(train_data, 'train', THRESHOLD, MAX_LEN)\n","    train_loader = torch.utils.data.DataLoader(train_Ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n","\n","    test_Ds = TextDataset(test_data, 'test', THRESHOLD, MAX_LEN, train_Ds.idx2word, train_Ds.word2idx)\n","    test_loader = torch.utils.data.DataLoader(test_Ds, batch_size=1, shuffle=False, num_workers=1, drop_last=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lp5pAz8emxi2"},"source":["Now you can instantiate your model. We provide you with some recommended hyperparameters; you should be able to get the desired accuracy with these, but feel free to play around with them."]},{"cell_type":"code","metadata":{"id":"CA-UairGErap","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634236035181,"user_tz":300,"elapsed":9,"user":{"displayName":"Heejun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02999255376817400603"}},"outputId":"cddb4296-0436-4ec9-87d3-40520c986adc"},"source":["if __name__=='__main__':\n","    rnn_model = RNN(vocab_size = train_Ds.vocab_size, # Don't change this\n","                embed_size = 128, \n","                hidden_size = 128, \n","                num_layers = 2,\n","                bidirectional = True,\n","                dropout = 0.5,\n","                num_classes = 2, # Don't change this\n","                pad_idx = train_Ds.word2idx[PAD]) # Don't change this\n","\n","    # Put your model on device\n","    rnn_model = rnn_model.to(device)\n","\n","    print('The model has {:,d} trainable parameters'.format(count_parameters(rnn_model)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 4,300,546 trainable parameters\n"]}]},{"cell_type":"markdown","metadata":{"id":"LqngFY4MoLec"},"source":["Here, we create the criterion and optimizer; as with the CNN, we use cross-entropy loss and Adam optimization."]},{"cell_type":"code","metadata":{"id":"em6Rs58OlJ3Z"},"source":["if __name__=='__main__':    \n","    LEARNING_RATE = 5e-4 # Feel free to try other learning rates\n","\n","    # Define your loss function\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    # Define your optimizer\n","    optimizer = optim.Adam(rnn_model.parameters(), lr=LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uEPsi3choUm5"},"source":["Finally, we can train the model. We use the same `train_model(...)` function that we defined for the CNN."]},{"cell_type":"code","metadata":{"id":"NR8Wckf0l2G7","colab":{"base_uri":"https://localhost:8080/","height":376,"referenced_widgets":["336a98b6495840f59b0498b39b6d0036","92d97e76363641d999193d89a1e5bba8","e146bfa504e94bf2a32b83f2b6caa9a3","b61c99316729458a806f242e5d04037f","9ce61878c63d49f0a558dd261214af53","a92b423aaf7b4a46a9fb0d7fe924598d","6e19338c3a4b442990ababb730557dd1","1bb33dfdefe34565a88e77fa60d7938e","efce1672901549f7982dc343f92329aa","a628d32a8ffc45cf87efd769f9a2252d","00945f895a9f4e9d8d03200c3b5f9ad1"]},"executionInfo":{"status":"ok","timestamp":1634239678738,"user_tz":300,"elapsed":3643562,"user":{"displayName":"Heejun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02999255376817400603"}},"outputId":"f6acb797-50ce-4ed5-f913-b5950a1ee10c"},"source":["if __name__=='__main__':    \n","    N_EPOCHS = 15 # Feel free to change this\n","    \n","    # train model for N_EPOCHS epochs\n","    train_model(rnn_model, N_EPOCHS, train_loader, optimizer, criterion)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Model...\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"336a98b6495840f59b0498b39b6d0036","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  1\t Loss: 0.6705\t Train Accuracy: 58.58%\n","[TRAIN]\t Epoch:  2\t Loss: 0.5492\t Train Accuracy: 72.52%\n","[TRAIN]\t Epoch:  3\t Loss: 0.4298\t Train Accuracy: 80.80%\n","[TRAIN]\t Epoch:  4\t Loss: 0.3417\t Train Accuracy: 85.76%\n","[TRAIN]\t Epoch:  5\t Loss: 0.2666\t Train Accuracy: 89.56%\n","[TRAIN]\t Epoch:  6\t Loss: 0.2010\t Train Accuracy: 92.50%\n","[TRAIN]\t Epoch:  7\t Loss: 0.1421\t Train Accuracy: 95.03%\n","[TRAIN]\t Epoch:  8\t Loss: 0.0958\t Train Accuracy: 96.84%\n","[TRAIN]\t Epoch:  9\t Loss: 0.0581\t Train Accuracy: 98.14%\n","[TRAIN]\t Epoch: 10\t Loss: 0.0473\t Train Accuracy: 98.40%\n","[TRAIN]\t Epoch: 11\t Loss: 0.0356\t Train Accuracy: 98.88%\n","[TRAIN]\t Epoch: 12\t Loss: 0.0275\t Train Accuracy: 99.07%\n","[TRAIN]\t Epoch: 13\t Loss: 0.0227\t Train Accuracy: 99.23%\n","[TRAIN]\t Epoch: 14\t Loss: 0.0219\t Train Accuracy: 99.30%\n","[TRAIN]\t Epoch: 15\t Loss: 0.0134\t Train Accuracy: 99.56%\n","Model Trained!\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"j-SRIFfooYk6"},"source":["## Evaluate RNN Model [20 points]\n","\n","Now we can evaluate the RNN. \n","\n","To pass the autograder for the RNN, you will need to achieve **75% accuracy** on the test set. Note that Gradescope uses a different test set; however, it is very similar, and the accuracies between the two datasets should be comparable."]},{"cell_type":"code","metadata":{"id":"HYon4AbHl5_M","colab":{"base_uri":"https://localhost:8080/","height":415,"referenced_widgets":["1b3213bbc4ca47b7af3f4a2a4d6fd833","d4f1191a0b5f4c82a7f611cc41f2f42e","aa8fc2a3001b46798e730ede0bf6d8d2","85c40bdafd7a4a6aa38ad04edfc44cf2","0c494cb905ab4dd0a492d40d137504b8","4c6fff20ce1b4016912229ecddd3a10b","6cef71ff599448308bf7dce98165b673","9489ea9684c443899620a08ce0beab4c","067d589352974a1da4dff5b4f5c3e17e","e4c2ee9c2a5240eab19c7aedc69edf27","b3b087d568f743daa5d5711417ecf2f8"]},"executionInfo":{"status":"ok","timestamp":1634239996922,"user_tz":300,"elapsed":209870,"user":{"displayName":"Heejun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02999255376817400603"}},"outputId":"37e58344-08f7-4e0c-ae39-26577db5b491"},"source":["if __name__=='__main__':    \n","    evaluate(rnn_model, test_loader, criterion) # Compute test data accuracy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating performance on the test dataset...\n","\n","SOME PREDICTIONS FROM THE MODEL:\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b3213bbc4ca47b7af3f4a2a4d6fd833","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Input: an okay film , with a fine leading lady , but a terrible leading man . <UNK> jenkins , who plays the husband , is a truly bad actor . joyce <UNK> , on the other hand , is the movie's saving grace . she's the best actor of the <UNK> /><br <UNK> the first comment , by the fellow who heaped praise upon the movie ( and , according to his imdb.com account , has only written one review -- and guess for what movie? ) is obviously a plant . while the movie is nicely shot , it's\n","Prediction: 0 \tCorrect Output: 0 \n","\n","Input: i saw this film last night following a lot of good reviews from many sources . i would like to point out that if your not ready to try and work out continuously who is who and what it all means you will hate this film.<br /><br />i am still struggling to understand the roles of the actors in this film , the film jumps from different stories and does not allow you to really empathise with any of the roles.<br /><br />for the political <UNK> and those interested in corruption in other world governments out there this film is\n","Prediction: 1 \tCorrect Output: 0 \n","\n","Input: the night listener is probably not one of william's best roles , but he makes a very interesting character in a somewhat odd but very different movie . i can guarantee you that you have never seen this kind of movie before . some people maybe won't like the slow pacing of this movie , but i think it's the great plus of the movie . it is definitely one of the top movies that have come out the year 2006 . it has a intriguing performance in a movie with a great content , dramatic feeling . this is\n","Prediction: 1 \tCorrect Output: 1 \n","\n","Input: what another reviewer called lack of character development , i call understatement . the movie didn't bash one over the head with <UNK> or unnecessary backstory . yes , there were many untold stories that we only got a glimpse of , but this was primarily a <UNK> snapshot into an event that <UNK> change in all of the characters ' lives . henry <UNK> performance was a really lovely study in the power of acting that focuses on reaction rather than action . good rental .\n","Prediction: 1 \tCorrect Output: 1 \n","\n","Input: you know , this movie isn't that great , but , i mean , c'mon , it's about angels helping a baseball team . i find the plot line to be hilarious anyways , this kid's dad says he'll take him back if the angels win the <UNK> ( because he knows they won't ) kid prays to his fake god to help the angels win , god helps the whole time ( via the angel christopher lloyd , rip ) and in the end , his dad doesn't take him back and rides off on his motorcycle right in\n","Prediction: 1 \tCorrect Output: 1 \n","\n","[TEST]\t Loss: 1.7876\t Accuracy: 76.96%\n"]}]},{"cell_type":"markdown","metadata":{"id":"8WQAV6O2xHvS"},"source":["# What You Need to Submit\n","\n","To submit the assignment, download this notebook as a <TT>.py</TT> file. You can do this by going to <TT>File > Download > Download .py</TT>. Then rename it to `hwk2.py`.\n","\n","You will also need to save the `cnn_model` and `rnn_model`. You can run the cell below to do this. After you save the files to your Google Drive, you need to manually download the files to your computer, and then submit them to the autograder.\n","\n","You will submit the following files to the autograder:\n","1.   `hwk2.py`, the download of this notebook as a `.py` file (**not** a `.ipynb` file)\n","1.   `cnn.pt`, the saved version of your `cnn_model`\n","1.   `rnn.pt`, the saved version of your `rnn_model`"]},{"cell_type":"code","metadata":{"id":"abbbMNi8X_ai","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634240025163,"user_tz":300,"elapsed":22136,"user":{"displayName":"Heejun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02999255376817400603"}},"outputId":"ef9cf916-ba00-45cb-aa3c-6e46f1b6c595"},"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    print()\n","\n","    try:\n","        cnn_model is None\n","        cnn_exists = True\n","    except:\n","        cnn_exists = False\n","\n","    try:\n","        rnn_model is None\n","        rnn_exists = True\n","    except:\n","        rnn_exists = False\n","\n","    if cnn_exists:\n","        print(\"Saving CNN model....\") \n","        torch.save(cnn_model, \"drive/My Drive/cnn.pt\")\n","    if rnn_exists:\n","        print(\"Saving RNN model....\") \n","        torch.save(rnn_model, \"drive/My Drive/rnn.pt\")\n","    print(\"Done!\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","Saving RNN model....\n","Done!\n"]}]}]}
